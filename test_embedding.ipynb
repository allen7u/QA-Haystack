{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and AARCH64. Falling back to GPT2TokenizerFast.\n",
      "WARNING:haystack.nodes.answer_generator.openai:OpenAI tiktoken module is not available for Python < 3.8,Linux ARM64 and AARCH64. Falling back to GPT2TokenizerFast.\n"
     ]
    }
   ],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http\n",
    "\n",
    "doc_dir = \"lecun-2022-path\" + '/'\n",
    "doc_dir = \"test\" + '/'\n",
    "\n",
    "doc = convert_files_to_docs(dir_path=doc_dir, split_paragraphs=False)\n",
    "# clean_func=clean_wiki_text, \n",
    "# print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c783c858e78484dbb1bff15224d58ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/1 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document: id=2b8d1f62cd0c79b4892a1e15b3772ced, content='DocumentStore\n",
      "You can think of the DocumentStore as a database that stores your texts and meta data ...'>\n",
      "[<Document: {'content': 'DocumentStore\\nYou can think of the DocumentStore as a database that stores your texts and meta data and provides them to the Retriever at query time. Learn how to choose the best DocumentStore for your use case and how to use it in a pipeline.\\n\\nSuggest Edits\\n\\nUse with Retrievers\\n\\nBy far the most common way to use a DocumentStore in Haystack is to fetch documents using a Retriever. You provide a DocumentStore as an argument when you initialize a Retriever.\\n\\nInitialization\\nTo use a DocumentStore in a pipeline, you must initialize it first. Initializing a new DocumentStore in Haystack is straightforward. Have a look at the instructions for different types of DocumentStores:\\n\\nclean_header_footer: Use heuristic to remove footers and headers across different pages by searching for the longest common string. This heuristic uses exact matches and therefore works well for footers like \"Copyright 2019 by XXX\", but won\\'t detect \"Page 3 of 4\" or similar.\\nclean_whitespace: Strip whitespaces before or after each line in the text.\\nclean_empty_lines: Remove more than two empty lines in the text.\\nremove_substrings: Remove specified substrings from the text. If no value is provided an empty list is created by default.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'test.txt', '_split_id': 0, 'page': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b8d1f62cd0c79b4892a1e15b3772ced'}>, <Document: {'content': 'This heuristic uses exact matches and therefore works well for footers like \"Copyright 2019 by XXX\", but won\\'t detect \"Page 3 of 4\" or similar.\\nclean_whitespace: Strip whitespaces before or after each line in the text.\\nclean_empty_lines: Remove more than two empty lines in the text.\\nremove_substrings: Remove specified substrings from the text. If no value is provided an empty list is created by default.\\nsplit_by: Unit for splitting the document. Can be \"word\", \"sentence\", or \"passage\". Set to None to disable splitting.\\nsplit_length: Max. number of the above split unit (e.g. words) that are allowed in one document. For instance, if n -> 10 & split_by -> \"sentence\", then each output document will have 10 sentences.\\nsplit_overlap: Word overlap between two adjacent documents after a split. Setting this to a positive number essentially enables the sliding window approach. For example, if split_by -> word, split_length -> 5 & split_overlap -> 2, then the splits would be like: [w1 w2 w3 w4 w5, w4 w5 w6 w7 w8, w7 w8 w10 w11 w12]. Set the value to 0 to ensure there is no overlap among the documents after splitting.', 'content_type': 'text', 'score': None, 'meta': {'name': 'test.txt', '_split_id': 1, 'page': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2fd6f74b7325080b200f38ce80e3e1c9'}>]\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length= 200,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap= 50,\n",
    "    add_page_number=True\n",
    ")\n",
    "# docs = [{'content': 'This is a sample content. It contains multiple sentences.'},\n",
    "        # {'content': 'This is another sample content. It contains multiple sentences.'}]\n",
    "docs = processor.process( doc )\n",
    "print(docs[0])\n",
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore(sql_url=\"sqlite:///\"+ doc_dir +\"my_db.db\", faiss_index_factory_str=\"Flat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd31adafa6c4429b03f65019ec1e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cdc9964c424160897912b6d7e4e100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/2 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970fd262eed2492194454d5e47821358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n",
    "    model_format=\"sentence_transformers\",\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "# document_store.save(\"my_document_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.save(doc_dir+\"my_faiss_index.faiss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "752663de3af262fa61ccfcf959a1b144fb276aa0351d143c8ec16a14a646eae1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
